---
author: Yan Wang
date: 2017-12-02 21:21:21 -0700
layout: post
title: NIPS 2017 阅读笔记
categories: [论文]
comments: true
tags:
-  论文
-  技术
-  科研
---

录用论文列表 ：[NIPS 2017 Accepted papers](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017)

## Hash Embeddings for Efficient Word Representations

兴趣点：Hash embedding, efficient


## Attention Is All You Need

兴趣点：Attention, No Recurrent

本文简直就是启发策略大合集，整体结构还算简明，大量篇幅介绍模型中的各种小 trick。整体结构来看，encoder 和 decoder 分别用多层 multi-head attention （其中 decode 中加入了一个 masking 用于消除 illegal connection），相邻层之间传递残差 （带 dropout），如果模型展开的话确实像变形金刚的两条腿，也不愧叫做 Transformer。这个模型结构的好处是，通过取消 recurrent 层，提高计算的并行度，同时计算量也明显减少，适合用 GPU 或者 TPU。至于性能嘛……光在机器翻译的公开数据集上测试，我是不太信的。

细节的 trick 主要包括：

* 在计算 attention 的时候加入了一个分母（维度的平方根），生成这是为了弥合 multiplicative attention 和 additive attention 之间的性能差异。同一个分母也加入到了每个 attention 层内部的 softmax 之前。
* 位置编码，用正余弦函数来编码位置，限定了一个波长范围，这个函数有个特点，如果两个位置之间有固定的 offset，那么这两个位置的编码之间存在线性关系。好处就是，测试的序列长度可以比训练的序列更长。

知乎传送门：[如何理解谷歌团队的机器翻译新作《Attention is all you need》？](https://www.zhihu.com/question/61077555)

