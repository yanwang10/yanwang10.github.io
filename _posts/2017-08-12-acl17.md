---
author: Yan Wang
date: 2017-06-13 21:21:21 -0700
layout: post
title: ACL 2017 阅读笔记
categories: [论文]
tags:
-  论文
-  技术
-  科研
---

录用论文列表 ：[ACL 2017 Accepted papers](https://chairs-blog.acl2017.org/2017/04/05/accepted-papers-and-demonstrations/)

只浏览了 Long Paper 的几个section。

# IE QA Text Mining Applications

## Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix

兴趣点：Noise in Relation Extraction


## Neural Relation Extraction with Multi-lingual Attention

兴趣点：Yankai Lin, Zhiyuan Liu

## Reading Wikipedia to Answer Open-Domain Questions

兴趣点：Wikipedia, Danqi Chen


# Semantics

## Improved Word Representation Learning with Sememes

兴趣点：Zhiyuan Liu

## Riemannian Optimization for Skip-Gram Negative Sampling

兴趣点：Negative Sampling


## Skip-Gram – Zipf + Uniform = Vector Additivity

兴趣点：Vector Additivity

本文没有提到实验结果，纯理论分析。首先分析了 Skip-Gram 中的 Compositionality 问题，发现当**词的分布更近均匀分布的时候，得到的词向量可加性更好**（一般语料中词频应该是 Zipf 分布）。这里所谓的可加性，是指对于一个包含若干词的 Context，用词向量的和来表示这个 Context 的语义。然后考虑如何把 Context 的语义投射回单词表中，理论上要求分布尽可能相同（公式7），但满足可加性之后，cosine similarity 就是一个很好的 proxy，也就是说，单词表中与 Context Vector 的 cosine similarity 最大的词，就是最可能的 paraphrase。

后面将 Skip-Gram 与 Sufficient Dimentionality Reduction 框架做了对比，找出了一些关联之处，由 SDR 的信息论最优推出 Skip-Gram 的最优性，没有详细读。
