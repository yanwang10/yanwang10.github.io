<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh" lang="zh">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.40.1" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>NIPS 2017 阅读笔记 &middot; 碎碎念</title>

  
  <link type="text/css" rel="stylesheet" href="https://yanwang10.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://yanwang10.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://yanwang10.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://yanwang10.github.io/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="碎碎念" />

  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://yanwang10.github.io/"><h1>碎碎念</h1></a>
      <p class="lead">
       偶尔碎碎念 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://yanwang10.github.io/">Home</a> </li>
        
      </ul>
    </nav>

    <p>&copy; 2019. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>NIPS 2017 阅读笔记</h1>
  <time datetime=2017-12-02T21:21:21-0700 class="post-date">Sat, Dec 2, 2017</time>
  

<p>录用论文列表 ：<a href="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017">NIPS 2017 Accepted papers</a></p>

<h2 id="hash-embeddings-for-efficient-word-representations">Hash Embeddings for Efficient Word Representations</h2>

<p>兴趣点：Hash embedding, efficient</p>

<h2 id="attention-is-all-you-need">Attention Is All You Need</h2>

<p>兴趣点：Attention, No Recurrent</p>

<p>本文简直就是启发策略大合集，整体结构还算简明，大量篇幅介绍模型中的各种小 trick。整体结构来看，encoder 和 decoder 分别用多层 multi-head attention （其中 decode 中加入了一个 masking 用于消除 illegal connection），相邻层之间传递残差 （带 dropout），如果模型展开的话确实像变形金刚的两条腿，也不愧叫做 Transformer。这个模型结构的好处是，通过取消 recurrent 层，提高计算的并行度，同时计算量也明显减少，适合用 GPU 或者 TPU。至于性能嘛……光在机器翻译的公开数据集上测试，我是不太信的。</p>

<p>细节的 trick 主要包括：</p>

<ul>
<li>在计算 attention 的时候加入了一个分母（维度的平方根），生成这是为了弥合 multiplicative attention 和 additive attention 之间的性能差异。同一个分母也加入到了每个 attention 层内部的 softmax 之前。</li>
<li>位置编码，用正余弦函数来编码位置，限定了一个波长范围，这个函数有个特点，如果两个位置之间有固定的 offset，那么这两个位置的编码之间存在线性关系。好处就是，测试的序列长度可以比训练的序列更长。</li>
</ul>

<p>知乎传送门：<a href="https://www.zhihu.com/question/61077555">如何理解谷歌团队的机器翻译新作《Attention is all you need》？</a></p>

</div>


    </main>

    
  </body>
</html>
