---
author: Yan Wang
categories:
  - 论文
date: 2017-06-13 21:21:21 -0700
tags:
  - 论文
  - 技术
  - 科研
title: ACL 2017 阅读笔记
url: /2017/08/12/acl17/
---


录用论文列表 ：[ACL 2017 Accepted papers](https://chairs-blog.acl2017.org/2017/04/05/accepted-papers-and-demonstrations/)

只浏览了 Long Paper 的几个section。

# IE QA Text Mining Applications

## Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix

兴趣点：Noise in Relation Extraction

没看懂这篇文章想干啥，只大概看了下核心部分，后面看不下去了。在正常训练过程中，模型会对每一个句子输出在各个 relation 上的概率分布 $$p$$，本文提出同时优化一个表征噪音的矩阵 $$T$$，即 Transition Matrix，第 i 行第 j 列的元素表示第 i 个 relation 错误地被标为 j 的概率。这个矩阵是对每个句子都计算的，训练过程中用 $$T^T \times p$$ 归一化之后的结果作为输出的分布，用于计算损失函数；测试过程中不考虑 $$T$$ 直接用 $$p$$ 作为输出。

很好奇多数情况下 $$T$$ 到底有多稠密，这个矩阵不一定会刻画噪音，没准会学出来一些不同 relation 之间的相关性，对于直接学习的模型输出进行调整。以及这个额外的计算量正比于支持的 relation 数量的平方，复杂度也是个问题，或许可以通过先验知识限制一下非零元素的个数。

不太了解 TimeRE 这个评测集是什么样的，看到 0.9 precision 的时候能达到 0.8 recall，惊呆了，现在的数据都刷这么高了么。以及在 EntityRE 评测集上，avg 跟 avg_TM 区别不大，att_TM 只比 att 在 low recall high precision 的一端明显强一些，究竟是 TM 可以对 Distant Supervision 有帮助，还是 TM 可以修正掉 attention 的某些问题，有点可疑。

## Neural Relation Extraction with Multi-lingual Attention

兴趣点：Yankai Lin, Zhiyuan Liu

## Reading Wikipedia to Answer Open-Domain Questions

兴趣点：Wikipedia, Danqi Chen


# Semantics

## Improved Word Representation Learning with Sememes

兴趣点：Zhiyuan Liu

## Riemannian Optimization for Skip-Gram Negative Sampling

兴趣点：Negative Sampling


## Skip-Gram – Zipf + Uniform = Vector Additivity

兴趣点：Vector Additivity

本文没有提到实验结果，纯理论分析。首先分析了 Skip-Gram 中的 Compositionality 问题，发现当**词的分布更近均匀分布的时候，得到的词向量可加性更好**（一般语料中词频应该是 Zipf 分布）。这里所谓的可加性，是指对于一个包含若干词的 Context，用词向量的和来表示这个 Context 的语义。然后考虑如何把 Context 的语义投射回单词表中，理论上要求分布尽可能相同（公式7），但满足可加性之后，cosine similarity 就是一个很好的 proxy，也就是说，单词表中与 Context Vector 的 cosine similarity 最大的词，就是最可能的 paraphrase。

后面将 Skip-Gram 与 Sufficient Dimentionality Reduction 框架做了对比，找出了一些关联之处，由 SDR 的信息论最优推出 Skip-Gram 的最优性，没有详细读。