---
author: Yan Wang
categories:
  - 论文
date: 2017-06-13 21:21:21 -0700
tags:
  - 论文
  - 技术
  - 科研
title: KDD 2017 阅读笔记
url: /2017/08/12/kdd17/
---


录用论文列表 ：[KDD 2017 Accepted papers](http://www.kdd.org/kdd2017/accepted-papers)

只浏览了 RESEARCH TRACK PAPERS - ORAL 的部分。

## Learning certifiably optimal rule lists for categorical data

兴趣点：Rule learning

本文讲了一种在小规模数据+离散特征的条件下，学习 decision rule 模型的一种算法。符号记法比较复杂，有很多证明，匆匆扫了一眼。主要是用前缀树做了一些优化，证了一些跟前缀有关的定理，实验的数据太小了（$$O(10K)$$ 训练样例，$$O(10)$$ 个 categorical 特征维度，目测可以转化为 $$O(100)$$ 个 binary 特征）。不过 certifiably 确实是一个引人注目的特性。


## Local Higher-Order Graph Clustering

兴趣点：Jure Leskovec

看不懂= = 弃疗

## Discrete Content-aware Matrix Factorization

兴趣点：Matrix Factorization, Descrete


## Toeplitz Inverse Covariance-Based Clustering of Multivariate Time Series Data

兴趣点：Time Series, Jure Leskovec


## Linearized GMM Kernels and Normalized Random Fourier Features

兴趣点： Random Fourier Features


## Anomaly Detection with Robust Deep Auto-encoders

兴趣点：Anomaly Detection

"The key insight is that noise and outliers are essentially incompressible and therefore cannot effectively
be projected to a low-dimensional hidden layer by an autoencoder."

把输入特征矩阵 $$X$$ 分成两部分 $$X = L_D + S$$，其中 $$L_D$$ 是容易被 Autoencoder 以较少 hidden units 进行编码的部分（对应 Robust PCA 中被分解的 low-rank 矩阵），$$S$$ 是剩下的噪音和异常数据点，目标函数通过一个超参数来平衡 Autoencoder 的 reconstruction error 和 $$S$$ 矩阵中的非零项数 （或者松弛为某些矩阵范数）。

然后文章进一步将 "anomaly" 分为两种情况，一种是 anomalous features，即某一维特征在多个样例中出现异常（比如感光元件有坏点），另一种是 anomalous instances，即某一个样例的很多特征都异常（比如错误的 label，将 "7" 的图片错标为 "2"）。这两者的区别可以体现在目标函数里对 $$S$$ 矩阵范数的取法，是先对行取范数还是先对列取范数。

训练过程是迭代式的，每一个迭代里先训练 Autoencoder 重建 $$X - S$$ 得到重建结果 $$L_D$$，然后再用 Proximal operator 拟合 $$X - L_D$$ 得到新的 $$S$$，如果没达到收敛条件就再进行下一个迭代。不过那个判断收敛的条件有点神奇，没太看懂。

实验部分没仔细看，就是在 MNIST 数据集上做了一些实验，给出了一些可视化的样例，当给输入的图片添加较多噪声的时候，文章提出的 RDA (Robust Deep Autoencoder) 比标准的 DA 重建出的图片更干净，从而提升分类性能。

## Interpretable Predictions of Tree-based Ensembles via Actionable Feature Tweaking

兴趣点：Feature Tweaking


